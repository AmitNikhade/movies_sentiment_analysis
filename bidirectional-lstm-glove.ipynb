{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import Libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport os\nimport gc\nimport tqdm\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import * \nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.sequence import pad_sequences\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract Zip Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile \nwith ZipFile('../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', 'r') as zip:\n    zip.extractall() \nwith ZipFile('../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip', 'r') as zip:\n    zip.extractall() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import .tsv Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('./train.tsv', sep=\"\\t\")\ntest = pd.read_csv('./test.tsv', sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyse the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sentiment.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.SentenceId.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Balancing the Unbalanced Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_0 = train[train['Sentiment'] == 0].sample(frac=1)\ntrain_1 = train[train['Sentiment'] == 1].sample(frac=1)\ntrain_2 = train[train['Sentiment'] == 2].sample(frac=1)\ntrain_3 = train[train['Sentiment'] == 3].sample(frac=1)\ntrain_4 = train[train['Sentiment'] == 4].sample(frac=1)\n\n# we want a balanced set for training against - there are 7072 `0` examples\nsample_size = min(len(train_0), len(train_1), len(train_2), len(train_3), len(train_4))\n\ntrain = pd.concat([train_0.head(sample_size), train_1.head(sample_size), train_2.head(sample_size), train_3.head(sample_size), train_4.head(sample_size)]).sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sentence Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nfrom string import punctuation\nimport re\nfrom tqdm import tqdm\n\nstemmer=SnowballStemmer('english')\nlemma=WordNetLemmatizer()\n\ndef cleaner(phrase):\n    cleaned=[]\n    for i in tqdm(range(0,len(phrase))):\n        review=str(phrase[i])\n        review=re.sub('[^a-zA-Z]',' ',review)\n        #review=[stemmer.stem(w) for w in word_tokenize(str(review).lower())]\n        review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n        review=' '.join(review)\n        cleaned.append(review)\n    return cleaned\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cleaned_phrase']=cleaner(train.Phrase.values)\ntest['cleaned_phrase']=cleaner(test.Phrase.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain = train.drop(labels='Phrase',axis=1)\ntest = test.cleaned_phrase.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nX = train.cleaned_phrase.values\ny = train.Sentiment.values\ny =to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting to train validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.20,stratify=y,random_state=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maximum Features\nmax_features=len(FreqDist(word_tokenize(' '.join(X_train))))\nmax_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum sentence length\nlen_of_words=[]\nfor text in tqdm(X_train):\n    max_phrase_len=np.max(len_of_words.append(len(word_tokenize(text))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting text to seq and seq padding \n\ntokenizer = Tokenizer(num_words=max_features, oov_token='<unw>')\ntokenizer.fit_on_texts(list(X_train))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(X_train, maxlen=max_phrase_len)\n\nX_val = tokenizer.texts_to_sequences(X_val)\nX_val = sequence.pad_sequences(X_val, maxlen=max_phrase_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word GLove Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n    \ndef get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n    # word vectors\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    # embedding matrix\n    word_index = tokenizer.word_index\n    num_words = min(max_features, len(word_index) + 1)\n    all_embs = np.stack(embeddings_index.values()) #for random init\n    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n                                        (num_words, embed_dim))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    max_features = embedding_matrix.shape[0]\n    \n    return embedding_matrix\n\nEMBEDDING_FILE = '../input/glove6b100dtxt/glove.6B.100d.txt'\nembed_dim = 100 #word vector dim\nembedding_matrix = get_embed_mat(EMBEDDING_FILE,max_features,embed_dim)\nEMBEDDING_DIM = embed_dim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Creations with Bidirectional LSTM cells and Glove embeddings "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(max_features, EMBEDDING_DIM,weights=[embedding_matrix],input_length=X_train.shape[1], trainable=False))\nmodel.add(SpatialDropout1D(0.25))\n\nmodel.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64)))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(5,activation='softmax'))\n\nmodel.compile(optimizer=Adam(lr=0.001),\n loss='categorical_crossentropy',\n metrics=['accuracy'])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=5, batch_size=64, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the Accuracy and Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions on the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.read_csv('./test.tsv', sep=\"\\t\")\nt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = cleaner(t.Phrase.values)\ntest = tokenizer.texts_to_sequences(test)\ntest = sequence.pad_sequences(test, 48)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict_classes(test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('./test.tsv', sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Sentiment'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}