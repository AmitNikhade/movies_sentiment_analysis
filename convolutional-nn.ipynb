{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import Lib's"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport os\nimport gc\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import * \nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.sequence import pad_sequences\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extraxt and Import the dataset files"},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile \nwith ZipFile('../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', 'r') as zip:\n    zip.extractall() \nwith ZipFile('../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip', 'r') as zip:\n    zip.extractall() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('./train.tsv', sep=\"\\t\")\ntest = pd.read_csv('./test.tsv', sep=\"\\t\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyse the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sentiment.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sentiment.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.SentenceId.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handing the Imbalanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_0 = train[train['Sentiment'] == 0].sample(frac=1)\ntrain_1 = train[train['Sentiment'] == 1].sample(frac=1)\ntrain_2 = train[train['Sentiment'] == 2].sample(frac=1)\ntrain_3 = train[train['Sentiment'] == 3].sample(frac=1)\ntrain_4 = train[train['Sentiment'] == 4].sample(frac=1)\n\n# we want a balanced set for training against - there are 7072 `0` examples\nsample_size = min(len(train_0), len(train_1), len(train_2), len(train_3), len(train_4))\n\ntrain = pd.concat([train_0.head(sample_size), train_1.head(sample_size), train_2.head(sample_size), train_3.head(sample_size), train_4.head(sample_size)]).sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sentence cleaner"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence Cleaning\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nfrom string import punctuation\nimport re\nfrom tqdm import tqdm\n\nstemmer=SnowballStemmer('english')\nlemma=WordNetLemmatizer()\n\ndef cleaner(phrase):\n    cleaned=[]\n    for i in tqdm(range(0,len(phrase))):\n        review=str(phrase[i])\n        review=re.sub('[^a-zA-Z]',' ',review)\n        #review=[stemmer.stem(w) for w in word_tokenize(str(review).lower())]\n        review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n        review=' '.join(review)\n        cleaned.append(review)\n    return cleaned\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cleaned_phrase']=cleaner(train.Phrase.values)\ntest['cleaned_phrase']=cleaner(test.Phrase.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(labels='Phrase',axis=1)\ntest = test.cleaned_phrase.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nX = train.cleaned_phrase.values\ny = train.Sentiment.values\ny = to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into train test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.20,stratify=y,random_state=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maximum Features\nmax_features=len(FreqDist(word_tokenize(' '.join(X_train))))\nmax_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum sentence length\nlen_of_words=[]\nfor text in tqdm(X_train):\n    max_phrase_len=np.max(len_of_words.append(len(word_tokenize(text))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting text to seq and seq padding \n\ntokenizer = Tokenizer(num_words=max_features, oov_token='<unw>')\ntokenizer.fit_on_texts(list(X_train))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(X_train, maxlen=max_phrase_len)\n\nX_val = tokenizer.texts_to_sequences(X_val)\nX_val = sequence.pad_sequences(X_val, maxlen=max_phrase_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling with Convolutional layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"model= Sequential()\n\nmodel.add(Embedding(max_features,100,input_length=48, mask_zero = True))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(5,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitiing the data\nhistory=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=5, batch_size=64, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance Plotting"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions on the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.read_csv('./test.tsv', sep=\"\\t\")\nt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = cleaner(t.Phrase.values)\ntest = tokenizer.texts_to_sequences(test)\ntest = sequence.pad_sequences(test, 48)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict_classes(test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('./test.tsv', sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Sentiment'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}